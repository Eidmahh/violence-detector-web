{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837dda09-d0c6-4f03-83b4-72a6de9ca12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b4f153-32a2-408a-a435-c05516f89192",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 405\u001b[0m\n\u001b[0;32m    403\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[0;32m    404\u001b[0m     detector\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[1;32m--> 405\u001b[0m     detector\u001b[38;5;241m.\u001b[39mtrain_model(X, y)\n\u001b[0;32m    407\u001b[0m detector\u001b[38;5;241m.\u001b[39mreal_time_detection()\n",
      "Cell \u001b[1;32mIn[4], line 282\u001b[0m, in \u001b[0;36mViolenceDetector.train_model\u001b[1;34m(self, X, y, epochs, batch_size)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m--> 282\u001b[0m     X_tr, X_te, y_tr, y_te \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    283\u001b[0m     cw \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m5\u001b[39m}\n\u001b[0;32m    284\u001b[0m     es \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2780\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2777\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2779\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2780\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2781\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2782\u001b[0m )\n\u001b[0;32m   2784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2410\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2407\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2412\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2413\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2414\u001b[0m     )\n\u001b[0;32m   2416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "ons(\n",
    "            base_options=base_options,\n",
    "            running_mode=vision.RunningMode.IMAGE,\n",
    "            num_poses=self.max_people,\n",
    "            min_pose_detection_confidence=0.5\n",
    "        )\n",
    "        self.pose_image = vision.PoseLandmarker.create_from_options(image_options)\n",
    "\n",
    "    def extract_person_features(self, landmarks_dict):\n",
    "        try:\n",
    "            def get_angle(p1, p2, p3):\n",
    "                v1 = p1 - p2\n",
    "                v2 = p3 - p2\n",
    "                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "                angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "                return np.degrees(angle)\n",
    "            \n",
    "            center = (landmarks_dict['LEFT_HIP'] + landmarks_dict['RIGHT_HIP']) / 2\n",
    "            return np.array([\n",
    "                get_angle(landmarks_dict['LEFT_SHOULDER'],\n",
    "                        landmarks_dict['LEFT_ELBOW'],\n",
    "                        landmarks_dict['LEFT_WRIST']),\n",
    "                get_angle(landmarks_dict['RIGHT_SHOULDER'],\n",
    "                        landmarks_dict['RIGHT_ELBOW'],\n",
    "                        landmarks_dict['RIGHT_WRIST']),\n",
    "                get_angle(landmarks_dict['LEFT_HIP'],\n",
    "                        landmarks_dict['LEFT_KNEE'],\n",
    "                        landmarks_dict['LEFT_ANKLE']),\n",
    "                get_angle(landmarks_dict['RIGHT_HIP'],\n",
    "                        landmarks_dict['RIGHT_KNEE'],\n",
    "                        landmarks_dict['RIGHT_ANKLE']),\n",
    "                get_angle(landmarks_dict['LEFT_HIP'],\n",
    "                        landmarks_dict['RIGHT_HIP'],\n",
    "                        (landmarks_dict['LEFT_SHOULDER'] + landmarks_dict['RIGHT_SHOULDER']) / 2),\n",
    "                get_angle(landmarks_dict['NOSE'],\n",
    "                        (landmarks_dict['LEFT_SHOULDER'] + landmarks_dict['RIGHT_SHOULDER']) / 2,\n",
    "                        (landmarks_dict['LEFT_HIP'] + landmarks_dict['RIGHT_HIP']) / 2),\n",
    "                np.linalg.norm(landmarks_dict['RIGHT_SHOULDER'] - landmarks_dict['LEFT_SHOULDER']),\n",
    "                np.linalg.norm(landmarks_dict['RIGHT_HIP'] - landmarks_dict['LEFT_HIP']),\n",
    "                np.linalg.norm(center - self.tracker.objects.get('last_position', center))\n",
    "            ]), center\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Feature extraction error: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to read image: {image_path}\")\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
    "            detection_result = self.pose_image.detect(mp_image)\n",
    "            return detection_result\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def preprocess_dataset(self, force_reprocess=False):\n",
    "        if os.path.exists(self.features_cache_path) and not force_reprocess:\n",
    "            with open(self.features_cache_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            return data['X'], data['y']\n",
    "        \n",
    "        X, y = [], []\n",
    "        for label in ['normal', 'violence']:\n",
    "            class_path = self.dataset_paths[label]\n",
    "            for video_folder in sorted(os.listdir(class_path)):\n",
    "                video_path = os.path.join(class_path, video_folder)\n",
    "                if not os.path.isdir(video_path):\n",
    "                    continue\n",
    "                \n",
    "                video_features = []\n",
    "                for frame_file in sorted(os.listdir(video_path)):\n",
    "                    if not frame_file.lower().endswith(('.jpg','.png','.jpeg')):\n",
    "                        continue\n",
    "                    \n",
    "                    frame_path = os.path.join(video_path, frame_file)\n",
    "                    result = self.preprocess_image(frame_path)\n",
    "                    if result and result.pose_landmarks:\n",
    "                        for pose in result.pose_landmarks:\n",
    "                            ld_dict = {mp.solutions.pose.PoseLandmark(i).name: \n",
    "                                      np.array([lm.x, lm.y]) \n",
    "                                      for i, lm in enumerate(pose)}\n",
    "                            feats, _ = self.extract_person_features(ld_dict)\n",
    "                            if feats is not None:\n",
    "                                video_features.append(feats)\n",
    "                \n",
    "                # Create sequences from video features\n",
    "                for i in range(len(video_features) - self.sequence_length + 1):\n",
    "                    X.append(video_features[i:i+self.sequence_length])\n",
    "                    y.append(0 if label == 'normal' else 1)\n",
    "\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        with open(self.features_cache_path, 'wb') as f:\n",
    "            pickle.dump({'X': X, 'y': y}, f)\n",
    "        return X, y\n",
    "\n",
    "    def build_model(self):\n",
    "        inp = Input((self.sequence_length, 9))  # Updated input shape\n",
    "        lstm_out = LSTM(128, return_sequences=True)(inp)\n",
    "        att = Dense(1, activation='tanh')(lstm_out)\n",
    "        att = SqueezeLayer(axis=-1)(att)\n",
    "        att_w = Softmax(axis=1)(att)\n",
    "        att_w = ExpandDimsLayer(axis=2)(att_w)\n",
    "        ctxt = MultiplyLayer()([att_w, lstm_out])\n",
    "        ctxt = ReduceSumLayer(axis=1)(ctxt)\n",
    "        x = Dense(64, activation='relu')(ctxt)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        out = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inp, out)\n",
    "        model.compile('adam', 'binary_crossentropy', ['accuracy'])\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train_model(self, X, y, epochs=50, batch_size=32):\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        cw = {0: 1, 1: 5}\n",
    "        es = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        mc = tf.keras.callbacks.ModelCheckpoint(self.model_path, save_best_only=True)\n",
    "        self.model.fit(\n",
    "            X_tr, y_tr,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            class_weight=cw,\n",
    "            callbacks=[es, mc]\n",
    "        )\n",
    "        preds = (self.model.predict(X_te) > 0.5).astype(int)\n",
    "        print(\"Accuracy:\", accuracy_score(y_te, preds))\n",
    "        print(\"F1 Score:\", f1_score(y_te, preds))\n",
    "        print(confusion_matrix(y_te, preds))\n",
    "\n",
    "    def load_trained_model(self):\n",
    "        if os.path.exists(self.model_path):\n",
    "            self.model = load_model(\n",
    "                self.model_path,\n",
    "                custom_objects={\n",
    "                    'SqueezeLayer': SqueezeLayer,\n",
    "                    'ExpandDimsLayer': ExpandDimsLayer,\n",
    "                    'MultiplyLayer': MultiplyLayer,\n",
    "                    'ReduceSumLayer': ReduceSumLayer\n",
    "                }\n",
    "            )\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def real_time_detection(self, threshold=0.75, temporal_smoothing=10):\n",
    "        if not self.load_trained_model():\n",
    "            print(\"Train or load model first.\")\n",
    "            return\n",
    "        \n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open video capture\")\n",
    "            return\n",
    "        \n",
    "        plt.ion()\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        fig.canvas.manager.set_window_title('Violence Detection System')\n",
    "        buffer = deque(maxlen=self.sequence_length)\n",
    "        prediction_history = deque(maxlen=temporal_smoothing)\n",
    "        timestamp_ms = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Frame capture failed\")\n",
    "                    break\n",
    "                \n",
    "                # Process frame\n",
    "                timestamp_ms = int((time.time() - start_time) * 1000)\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "                \n",
    "                try:\n",
    "                    detection_result = self.pose_video.detect_for_video(mp_image, timestamp_ms)\n",
    "                    centroids = []\n",
    "                    features = []\n",
    "                    \n",
    "                    if detection_result.pose_landmarks:\n",
    "                        for pose in detection_result.pose_landmarks:\n",
    "                            ld_dict = {mp.solutions.pose.PoseLandmark(i).name: \n",
    "                                      np.array([lm.x, lm.y]) \n",
    "                                      for i, lm in enumerate(pose)}\n",
    "                            feat, centroid = self.extract_person_features(ld_dict)\n",
    "                            if feat is not None:\n",
    "                                features.append(feat)\n",
    "                                centroids.append(centroid)\n",
    "                    \n",
    "                    # Update tracker and get current objects\n",
    "                    tracked_objects = self.tracker.update(np.array(centroids))\n",
    "                    \n",
    "                    if tracked_objects and features:\n",
    "                        main_person_id = min(tracked_objects.keys())  # Track oldest person\n",
    "                        buffer.append(features[0])\n",
    "                    \n",
    "                    if len(buffer) == self.sequence_length:\n",
    "                        prediction = self.model.predict(np.array([list(buffer)]), verbose=0)[0][0]\n",
    "                        prediction_history.append(prediction)\n",
    "                        avg_pred = np.mean(prediction_history)\n",
    "                        \n",
    "                        # Update display\n",
    "                        status = \"VIOLENCE DETECTED!\" if avg_pred > threshold else \"Normal activity\"\n",
    "                        color = (255, 0, 0) if avg_pred > threshold else (0, 255, 0)\n",
    "                        cv2.putText(frame, status, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "                        cv2.putText(frame, f\"Score: {avg_pred:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 165, 0), 2)\n",
    "                    else:\n",
    "                        buffer.clear()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Processing error: {e}\")\n",
    "                \n",
    "                # Update matplotlib display\n",
    "                ax.clear()\n",
    "                ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                ax.axis('off')\n",
    "                plt.pause(0.001)\n",
    "                \n",
    "                if not plt.fignum_exists(fig.number):\n",
    "                    break\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopped by user\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            plt.ioff()\n",
    "            plt.close()\n",
    "            print(\"Detection stopped\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    detector = ViolenceDetector()\n",
    "    X, y = detector.preprocess_dataset()\n",
    "    \n",
    "    if not detector.load_trained_model():\n",
    "        tf.keras.backend.clear_session()\n",
    "        detector.build_model()\n",
    "        detector.train_model(X, y)\n",
    "    \n",
    "    detector.real_time_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c99bf-fa6b-4fb3-aedd-3e9a91b3e80a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Softmax, Layer\n",
    "import os\n",
    "import urllib.request\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Custom Layer Classes\n",
    "class SqueezeLayer(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.squeeze(inputs, axis=self.axis)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "        if isinstance(self.axis, int):\n",
    "            del output_shape[self.axis]\n",
    "        else:\n",
    "            for ax in sorted(self.axis, reverse=True):\n",
    "                del output_shape[ax]\n",
    "        return tuple(output_shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"axis\": self.axis})\n",
    "        return config\n",
    "\n",
    "class ExpandDimsLayer(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.expand_dims(inputs, axis=self.axis)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape.insert(self.axis, 1)\n",
    "        return tuple(output_shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"axis\": self.axis})\n",
    "        return config\n",
    "\n",
    "class MultiplyLayer(Layer):\n",
    "    def call(self, inputs):\n",
    "        return inputs[0] * inputs[1]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[1]\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "class ReduceSumLayer(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_sum(inputs, axis=self.axis)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape.pop(self.axis)\n",
    "        return tuple(output_shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"axis\": self.axis})\n",
    "        return config\n",
    "\n",
    "class ViolenceDetector:\n",
    "    def __init__(self, sequence_length=15, max_people=5):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_people = max_people\n",
    "        self.model = None\n",
    "        self.model_path = 'violence_detector_final.keras'\n",
    "        self.pose_landmarker_path = 'pose_landmarker.task'\n",
    "        self.features_cache_path = 'extracted_features.pkl'\n",
    "        self.previous_positions = {}\n",
    "        self.dataset_paths = {\n",
    "            'normal': 'violence_dataset/non_violence',\n",
    "            'violence': 'violence_dataset/violence'}\n",
    "        \n",
    "        # MediaPipe setup\n",
    "        if not os.path.exists(self.pose_landmarker_path):\n",
    "            print(\"Downloading pose landmarker model...\")\n",
    "            model_url = \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\"\n",
    "            urllib.request.urlretrieve(model_url, self.pose_landmarker_path)\n",
    "            print(\"Model downloaded successfully!\")\n",
    "            \n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        base_options = python.BaseOptions(model_asset_path=self.pose_landmarker_path)\n",
    "        options = vision.PoseLandmarkerOptions(\n",
    "            base_options=base_options,\n",
    "            running_mode=vision.RunningMode.IMAGE,\n",
    "            num_poses=self.max_people,\n",
    "            min_pose_detection_confidence=0.5\n",
    "        )\n",
    "        self.pose = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "    def extract_person_features(self, landmarks_dict, person_id):\n",
    "        try:\n",
    "            def get_angle(p1, p2, p3):\n",
    "                v1 = p1 - p2\n",
    "                v2 = p3 - p2\n",
    "                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "                angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "                return np.degrees(angle)\n",
    "            \n",
    "            left_arm_angle = get_angle(\n",
    "                landmarks_dict['LEFT_SHOULDER'],\n",
    "                landmarks_dict['LEFT_ELBOW'],\n",
    "                landmarks_dict['LEFT_WRIST']\n",
    "            )\n",
    "            right_arm_angle = get_angle(\n",
    "                landmarks_dict['RIGHT_SHOULDER'],\n",
    "                landmarks_dict['RIGHT_ELBOW'],\n",
    "                landmarks_dict['RIGHT_WRIST']\n",
    "            )\n",
    "            left_leg_angle = get_angle(\n",
    "                landmarks_dict['LEFT_HIP'],\n",
    "                landmarks_dict['LEFT_KNEE'],\n",
    "                landmarks_dict['LEFT_ANKLE']\n",
    "            )\n",
    "            right_leg_angle = get_angle(\n",
    "                landmarks_dict['RIGHT_HIP'],\n",
    "                landmarks_dict['RIGHT_KNEE'],\n",
    "                landmarks_dict['RIGHT_ANKLE']\n",
    "            )\n",
    "            pelvic_tilt_angle = get_angle(\n",
    "                landmarks_dict['LEFT_HIP'],\n",
    "                landmarks_dict['RIGHT_HIP'],\n",
    "                (landmarks_dict['LEFT_SHOULDER'] + landmarks_dict['RIGHT_SHOULDER']) / 2\n",
    "            )\n",
    "            spine_angle = get_angle(\n",
    "                landmarks_dict['NOSE'],\n",
    "                (landmarks_dict['LEFT_SHOULDER'] + landmarks_dict['RIGHT_SHOULDER']) / 2,\n",
    "                (landmarks_dict['LEFT_HIP'] + landmarks_dict['RIGHT_HIP']) / 2\n",
    "            )\n",
    "            left_leg_extension = get_angle(\n",
    "                landmarks_dict['LEFT_ANKLE'],\n",
    "                landmarks_dict['LEFT_KNEE'],\n",
    "                landmarks_dict['LEFT_HIP']\n",
    "            )\n",
    "            right_leg_extension = get_angle(\n",
    "                landmarks_dict['RIGHT_ANKLE'],\n",
    "                landmarks_dict['RIGHT_KNEE'],\n",
    "                landmarks_dict['RIGHT_HIP']\n",
    "            )\n",
    "            shoulder_width = np.linalg.norm(landmarks_dict['RIGHT_SHOULDER'] - landmarks_dict['LEFT_SHOULDER'])\n",
    "            hip_width = np.linalg.norm(landmarks_dict['RIGHT_HIP'] - landmarks_dict['LEFT_HIP'])\n",
    "            shoulder_hip_ratio = shoulder_width / (hip_width + 1e-6)\n",
    "            center = (landmarks_dict['LEFT_HIP'] + landmarks_dict['RIGHT_HIP']) / 2\n",
    "            velocity = np.linalg.norm(center - self.previous_positions.get(person_id, center))\n",
    "            self.previous_positions[person_id] = center\n",
    "            \n",
    "            return np.array([\n",
    "                left_arm_angle, right_arm_angle,\n",
    "                left_leg_angle, right_leg_angle,\n",
    "                pelvic_tilt_angle, spine_angle,\n",
    "                left_leg_extension, right_leg_extension,\n",
    "                shoulder_hip_ratio, velocity\n",
    "            ]), center\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Feature extraction error: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to read image: {image_path}\")\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
    "            detection_result = self.pose.detect(mp_image)\n",
    "            \n",
    "            if detection_result.pose_landmarks:\n",
    "                ld_dict = {self.mp_pose.PoseLandmark(i).name:\n",
    "                           np.array([lm.x, lm.y])\n",
    "                           for i, lm in enumerate(detection_result.pose_landmarks[0])}\n",
    "                return ld_dict\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def preprocess_dataset(self, force_reprocess=False):\n",
    "        if os.path.exists(self.features_cache_path) and not force_reprocess:\n",
    "            with open(self.features_cache_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            return data['X'], data['y']\n",
    "            \n",
    "        X, y = [], []\n",
    "        for label, folder in [('normal', 'non_violence'), ('violence', 'violence')]:\n",
    "            path = os.path.join('violence_dataset', folder)\n",
    "            for fn in sorted(os.listdir(path)):\n",
    "                if fn.lower().endswith(('.jpg','.png','.jpeg')):\n",
    "                    ld = self.preprocess_image(os.path.join(path, fn))\n",
    "                    if ld:\n",
    "                        feats, _ = self.extract_person_features(ld, 0)\n",
    "                        if feats is not None:\n",
    "                            X.append(feats)\n",
    "                            y.append(0 if label=='normal' else 1)\n",
    "                            \n",
    "        X, y = np.array(X), np.array(y)\n",
    "        seqs, labs = [], []\n",
    "        \n",
    "        for cls in [0, 1]:\n",
    "            idxs = np.where(y==cls)[0]\n",
    "            for i in range(len(idxs)-self.sequence_length+1):\n",
    "                seqs.append(X[idxs[i]:idxs[i]+self.sequence_length])\n",
    "                labs.append(cls)\n",
    "                \n",
    "        X, y = np.array(seqs), np.array(labs)\n",
    "        \n",
    "        with open(self.features_cache_path, 'wb') as f:\n",
    "            pickle.dump({'X': X, 'y': y}, f)\n",
    "            \n",
    "        return X, y\n",
    "\n",
    "    def build_model(self):\n",
    "        inp = Input((self.sequence_length, 10))\n",
    "        lstm_out = LSTM(128, return_sequences=True)(inp)\n",
    "        att = Dense(1, activation='tanh')(lstm_out)\n",
    "        att = SqueezeLayer(axis=-1)(att)\n",
    "        att_w = Softmax(axis=1)(att)\n",
    "        att_w = ExpandDimsLayer(axis=2)(att_w)\n",
    "        ctxt = MultiplyLayer()([att_w, lstm_out])\n",
    "        ctxt = ReduceSumLayer(axis=1)(ctxt)\n",
    "        x = Dense(64, activation='relu')(ctxt)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        out = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inp, out)\n",
    "        model.compile('adam', 'binary_crossentropy', ['accuracy'])\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train_model(self, X, y, epochs=50, batch_size=32):\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        cw = {0: 1, 1: 5}\n",
    "        es = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        mc = tf.keras.callbacks.ModelCheckpoint(self.model_path, save_best_only=True)\n",
    "        \n",
    "        self.model.fit(\n",
    "            X_tr, y_tr,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            class_weight=cw,\n",
    "            callbacks=[es, mc]\n",
    "        )\n",
    "        \n",
    "        preds = (self.model.predict(X_te) > 0.5).astype(int)\n",
    "        print(\"Accuracy:\", accuracy_score(y_te, preds))\n",
    "        print(\"F1 Score:\", f1_score(y_te, preds))\n",
    "        print(confusion_matrix(y_te, preds))\n",
    "\n",
    "    def load_trained_model(self):\n",
    "        if os.path.exists(self.model_path):\n",
    "            self.model = load_model(\n",
    "                self.model_path,\n",
    "                custom_objects={\n",
    "                    'SqueezeLayer': SqueezeLayer,\n",
    "                    'ExpandDimsLayer': ExpandDimsLayer,\n",
    "                    'MultiplyLayer': MultiplyLayer,\n",
    "                    'ReduceSumLayer': ReduceSumLayer\n",
    "                }\n",
    "            )\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def real_time_detection(self, threshold=0.75, temporal_smoothing=10):\n",
    "        \"\"\"Redesigned to use matplotlib for display instead of OpenCV\"\"\"\n",
    "        if self.model is None and not self.load_trained_model():\n",
    "            print(\"Train or load model first.\")\n",
    "            return\n",
    "            \n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open video capture\")\n",
    "            return\n",
    "            \n",
    "        buf = []\n",
    "        timestamp_ms = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(\"Starting real-time detection... Close the window to stop.\")\n",
    "        \n",
    "        # Set up matplotlib display\n",
    "        plt.ion()\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        fig.canvas.manager.set_window_title('Violence Detection System')\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Error: Frame capture failed\")\n",
    "                    break\n",
    "                \n",
    "                # Update timestamp\n",
    "                timestamp_ms = int((time.time() - start_time) * 1000)\n",
    "                \n",
    "                frm_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                try:\n",
    "                    det = self.pose.detect_for_video(\n",
    "                        mp.Image(image_format=mp.ImageFormat.SRGB, data=frm_rgb),\n",
    "                        timestamp_ms\n",
    "                    )\n",
    "                    \n",
    "                    if det.pose_landmarks:\n",
    "                        feats_list = []\n",
    "                        for i, plm in enumerate(det.pose_landmarks):\n",
    "                            ld = {self.mp_pose.PoseLandmark(j).name: np.array([l.x, l.y])\n",
    "                                  for j, l in enumerate(plm)}\n",
    "                            f, _ = self.extract_person_features(ld, i)\n",
    "                            if f is not None:\n",
    "                                feats_list.append(f)\n",
    "                        \n",
    "                        if feats_list:\n",
    "                            buf.append(feats_list[0])\n",
    "                            if len(buf) > self.sequence_length:\n",
    "                                buf.pop(0)\n",
    "                            \n",
    "                            if len(buf) == self.sequence_length:\n",
    "                                p = self.model.predict(np.array([buf]), verbose=0)[0][0]\n",
    "                                hist = getattr(self, 'prediction_history', []) + [p]\n",
    "                                if len(hist) > temporal_smoothing:\n",
    "                                    hist.pop(0)\n",
    "                                self.prediction_history = hist\n",
    "                                m = np.mean(hist)\n",
    "                                \n",
    "                                # Add detection results to frame\n",
    "                                txt = \"ALERT: Violence Detected\" if m > threshold else \"Normal Activity\"\n",
    "                                color = (255, 0, 0) if m > threshold else (0, 255, 0)\n",
    "                                cv2.putText(frame, txt, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "                                cv2.putText(frame, f\"Score: {m:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 165, 0), 2)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Processing error: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Display frame using matplotlib\n",
    "                ax.clear()\n",
    "                ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                ax.axis('off')\n",
    "                plt.pause(0.001)\n",
    "                \n",
    "                # Check if window was closed\n",
    "                if not plt.fignum_exists(fig.number):\n",
    "                    break\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopped by user\")\n",
    "            \n",
    "        finally:\n",
    "            # Clean up resources\n",
    "            cap.release()\n",
    "            plt.ioff()\n",
    "            plt.close()\n",
    "            print(\"Detection stopped\")\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'pose'):\n",
    "            self.pose.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    det = ViolenceDetector()\n",
    "    X, y = det.preprocess_dataset()\n",
    "    \n",
    "    if not det.load_trained_model():\n",
    "        tf.keras.backend.clear_session()\n",
    "        det.build_model()\n",
    "        det.train_model(X, y)\n",
    "    \n",
    "    det.real_time_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86eeb0-f945-41b4-880c-701ea5865ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\"\"\"\n",
    "this code for analyzing datasets to make sure if its good or no !!!! ++++++\n",
    "\"\"\"\n",
    "class ViolenceDatasetAnalyzer:\n",
    "    def __init__(self, violence_path, normal_path):\n",
    "        \"\"\"\n",
    "        Initialize with paths to violence and normal image folders\n",
    "        \n",
    "        Args:\n",
    "            violence_path (str): Path to directory containing violent images\n",
    "            normal_path (str): Path to directory containing normal images\n",
    "        \"\"\"\n",
    "        self.violence_path = violence_path\n",
    "        self.normal_path = normal_path\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_metadata(self):\n",
    "        \"\"\"Load dataset structure and count files in each category\"\"\"\n",
    "        self.classes = ['violence', 'normal']\n",
    "        self.file_counts = {\n",
    "            'violence': len(os.listdir(self.violence_path)),\n",
    "            'normal': len(os.listdir(self.normal_path))\n",
    "        }\n",
    "        self.total_samples = sum(self.file_counts.values())\n",
    "        \n",
    "        print(f\"Found {self.file_counts['violence']} violence samples\")\n",
    "        print(f\"Found {self.file_counts['normal']} normal samples\")\n",
    "        \n",
    "    def visualize_distribution(self):\n",
    "        \"\"\"Plot class distribution\"\"\"\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(x=list(self.file_counts.keys()), y=list(self.file_counts.values()))\n",
    "        plt.title('Class Distribution')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Add exact counts on top of bars\n",
    "        for i, count in enumerate(self.file_counts.values()):\n",
    "            plt.text(i, count, str(count), ha='center', va='bottom')\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate imbalance ratio\n",
    "        imbalance_ratio = max(self.file_counts.values()) / min(self.file_counts.values())\n",
    "        self.results['imbalance_ratio'] = imbalance_ratio\n",
    "        print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "        \n",
    "    def check_label_consistency(self, samples_per_class=5):\n",
    "        \"\"\"Display random samples from each class for visual verification\"\"\"\n",
    "        print(\"\\nVisual label verification:\")\n",
    "        \n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        # Violence samples\n",
    "        violence_files = np.random.choice(os.listdir(self.violence_path), samples_per_class)\n",
    "        for i, file in enumerate(violence_files):\n",
    "            img_path = os.path.join(self.violence_path, file)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                plt.subplot(2, samples_per_class, i+1)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Violence\\n{file[:15]}...\")\n",
    "                plt.axis('off')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {str(e)}\")\n",
    "        \n",
    "        # Normal samples\n",
    "        normal_files = np.random.choice(os.listdir(self.normal_path), samples_per_class)\n",
    "        for i, file in enumerate(normal_files):\n",
    "            img_path = os.path.join(self.normal_path, file)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                plt.subplot(2, samples_per_class, samples_per_class+i+1)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Normal\\n{file[:15]}...\")\n",
    "                plt.axis('off')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {str(e)}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def analyze_image_quality(self):\n",
    "        \"\"\"Check image resolution and blurriness for both classes\"\"\"\n",
    "        quality_metrics = {\n",
    "            'violence': {'resolution': [], 'blur': []},\n",
    "            'normal': {'resolution': [], 'blur': []}\n",
    "        }\n",
    "        \n",
    "        print(\"\\nAnalyzing image quality...\")\n",
    "        \n",
    "        # Process violence images\n",
    "        for file in tqdm(os.listdir(self.violence_path), desc=\"Violence images\"):\n",
    "            img_path = os.path.join(self.violence_path, file)\n",
    "            self._process_image(img_path, quality_metrics['violence'])\n",
    "        \n",
    "        # Process normal images\n",
    "        for file in tqdm(os.listdir(self.normal_path), desc=\"Normal images\"):\n",
    "            img_path = os.path.join(self.normal_path, file)\n",
    "            self._process_image(img_path, quality_metrics['normal'])\n",
    "        \n",
    "        # Store results\n",
    "        self.results['quality_metrics'] = quality_metrics\n",
    "        self._report_quality_metrics()\n",
    "        \n",
    "    def _process_image(self, img_path, metrics_dict):\n",
    "        \"\"\"Helper method to process individual images\"\"\"\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                return\n",
    "                \n",
    "            # Record resolution (height, width)\n",
    "            metrics_dict['resolution'].append(img.shape[:2])\n",
    "            \n",
    "            # Calculate blur metric (Laplacian variance)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            metrics_dict['blur'].append(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "    \n",
    "    def _report_quality_metrics(self):\n",
    "        \"\"\"Generate quality metrics report\"\"\"\n",
    "        quality = self.results['quality_metrics']\n",
    "        \n",
    "        print(\"\\n=== Quality Metrics ===\")\n",
    "        for class_name in ['violence', 'normal']:\n",
    "            res = np.mean(quality[class_name]['resolution'], axis=0)\n",
    "            blur_mean = np.mean(quality[class_name]['blur'])\n",
    "            \n",
    "            print(f\"\\n{class_name.capitalize()} Images:\")\n",
    "            print(f\"  Avg resolution: {res[0]:.0f}x{res[1]:.0f} (HxW)\")\n",
    "            print(f\"  Avg blur score: {blur_mean:.2f} (higher is better)\")\n",
    "            \n",
    "            # Store for overall report\n",
    "            self.results[f'{class_name}_avg_resolution'] = res\n",
    "            self.results[f'{class_name}_avg_blur'] = blur_mean\n",
    "            \n",
    "        # Plot comparative blur distribution\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.kdeplot(quality['violence']['blur'], label='Violence', shade=True)\n",
    "        sns.kdeplot(quality['normal']['blur'], label='Normal', shade=True)\n",
    "        plt.axvline(100, color='red', linestyle='--', label='Quality Threshold')\n",
    "        plt.title('Image Sharpness Comparison')\n",
    "        plt.xlabel('Blur Metric (Laplacian Variance)')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_full_report(self):\n",
    "        \"\"\"Generate comprehensive dataset report\"\"\"\n",
    "        print(\"\\n=== Dataset Quality Assessment Report ===\")\n",
    "        print(f\"Total samples: {self.total_samples}\")\n",
    "        print(f\"  Violence: {self.file_counts['violence']} ({self.file_counts['violence']/self.total_samples*100:.1f}%)\")\n",
    "        print(f\"  Normal: {self.file_counts['normal']} ({self.file_counts['normal']/self.total_samples*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nClass Imbalance Ratio: {self.results['imbalance_ratio']:.2f}:1\")\n",
    "        \n",
    "        print(\"\\nImage Quality Summary:\")\n",
    "        print(f\"Violence - Resolution: {self.results['violence_avg_resolution'][0]:.0f}x{self.results['violence_avg_resolution'][1]:.0f}\")\n",
    "        print(f\"          Blur Score: {self.results['violence_avg_blur']:.2f}\")\n",
    "        print(f\"Normal   - Resolution: {self.results['normal_avg_resolution'][0]:.0f}x{self.results['normal_avg_resolution'][1]:.0f}\")\n",
    "        print(f\"          Blur Score: {self.results['normal_avg_blur']:.2f}\")\n",
    "        \n",
    "        # Calculate overall quality score (0-100)\n",
    "        quality_score = 0\n",
    "        \n",
    "        # Balance score (max 30)\n",
    "        imbalance = self.results['imbalance_ratio']\n",
    "        if imbalance < 1.5:\n",
    "            quality_score += 30\n",
    "        elif imbalance < 3:\n",
    "            quality_score += 20\n",
    "        elif imbalance < 5:\n",
    "            quality_score += 10\n",
    "            \n",
    "        # Sample size score (max 20)\n",
    "        if self.total_samples >= 10000:\n",
    "            quality_score += 20\n",
    "        elif self.total_samples >= 5000:\n",
    "            quality_score += 15\n",
    "        elif self.total_samples >= 1000:\n",
    "            quality_score += 10\n",
    "        else:\n",
    "            quality_score += 5\n",
    "            \n",
    "        # Quality score (max 50)\n",
    "        min_blur = min(self.results['violence_avg_blur'], self.results['normal_avg_blur'])\n",
    "        if min_blur > 150:\n",
    "            quality_score += 50\n",
    "        elif min_blur > 100:\n",
    "            quality_score += 40\n",
    "        elif min_blur > 50:\n",
    "            quality_score += 25\n",
    "        else:\n",
    "            quality_score += 10\n",
    "            \n",
    "        print(f\"\\nOverall Dataset Quality Score: {quality_score}/100\")\n",
    "        \n",
    "        if quality_score >= 80:\n",
    "            print(\"✅ Excellent - Dataset is well-balanced and high quality\")\n",
    "        elif quality_score >= 60:\n",
    "            print(\"⚠️ Good - Dataset is usable but could be improved\")\n",
    "        else:\n",
    "            print(\"❌ Poor - Significant improvements needed before training\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify your folder paths here\n",
    "    VIOLENCE_PATH = \"C:/Users/user/Desktop/fridayEnhan/frames/normal\"\n",
    "    NORMAL_PATH = \"C:/Users/user/Desktop/fridayEnhan/frames/violence\"\n",
    "    \n",
    "    analyzer = ViolenceDatasetAnalyzer(\n",
    "        violence_path=VIOLENCE_PATH,\n",
    "        normal_path=NORMAL_PATH\n",
    "    )\n",
    "    \n",
    "    # Run analysis pipeline\n",
    "    analyzer.load_metadata()\n",
    "    analyzer.visualize_distribution()\n",
    "    analyzer.check_label_consistency()\n",
    "    analyzer.analyze_image_quality()\n",
    "    analyzer.generate_full_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee44f2-05fe-4ee5-b022-9a9b9ec69238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DatasetOptimizer:\n",
    "    def __init__(self, violence_path, normal_path, target_size=(480, 640)):\n",
    "        \"\"\"\n",
    "        this good for making teh data set better by augmenting and put filters etc!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        Args:\n",
    "            violence_path: Path to violence images\n",
    "            normal_path: Path to normal images  \n",
    "            target_size: Standard resolution (height, width)\n",
    "        \"\"\"\n",
    "        self.violence_path = violence_path\n",
    "        self.normal_path = normal_path\n",
    "        self.target_size = target_size\n",
    "        self.augmented_path = \"augmented_dataset\"\n",
    "        self.annotations_path = \"annotations\"\n",
    "        os.makedirs(self.augmented_path, exist_ok=True)\n",
    "        os.makedirs(self.annotations_path, exist_ok=True)\n",
    "\n",
    "    def analyze_dataset(self):\n",
    "        \"\"\"Initial quality check\"\"\"\n",
    "        print(\"Analyzing dataset...\")\n",
    "        violence_files = os.listdir(self.violence_path)\n",
    "        normal_files = os.listdir(self.normal_path)\n",
    "        \n",
    "        print(f\"\\nOriginal Counts:\")\n",
    "        print(f\"Violence: {len(violence_files)} images\")\n",
    "        print(f\"Normal: {len(normal_files)} images\")\n",
    "        print(f\"Imbalance Ratio: {len(violence_files)/len(normal_files):.2f}:1\")\n",
    "        \n",
    "        # Sample visualization\n",
    "        self._display_samples(violence_files, normal_files)\n",
    "\n",
    "    def _display_samples(self, violence_files, normal_files):\n",
    "        \"\"\"Show random samples\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Violence samples\n",
    "        for i, img_file in enumerate(np.random.choice(violence_files, 3)):\n",
    "            img = Image.open(os.path.join(self.violence_path, img_file))\n",
    "            plt.subplot(2, 3, i+1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Violence: {img_file[:15]}...\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        # Normal samples\n",
    "        for i, img_file in enumerate(np.random.choice(normal_files, 3)):\n",
    "            img = Image.open(os.path.join(self.normal_path, img_file))\n",
    "            plt.subplot(2, 3, i+4)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Normal: {img_file[:15]}...\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def standardize_resolution(self):\n",
    "        \"\"\"Resize all images to target dimensions\"\"\"\n",
    "        print(\"\\nStandardizing resolutions...\")\n",
    "        \n",
    "        # Process violence images\n",
    "        violence_output = os.path.join(self.augmented_path, \"violence\")\n",
    "        os.makedirs(violence_output, exist_ok=True)\n",
    "        \n",
    "        for img_file in tqdm(os.listdir(self.violence_path), desc=\"Violence images\"):\n",
    "            img = cv2.imread(os.path.join(self.violence_path, img_file))\n",
    "            resized = cv2.resize(img, (self.target_size[1], self.target_size[0]))\n",
    "            cv2.imwrite(os.path.join(violence_output, img_file), resized)\n",
    "        \n",
    "        # Process normal images\n",
    "        normal_output = os.path.join(self.augmented_path, \"normal\")\n",
    "        os.makedirs(normal_output, exist_ok=True)\n",
    "        \n",
    "        for img_file in tqdm(os.listdir(self.normal_path), desc=\"Normal images\"):\n",
    "            img = cv2.imread(os.path.join(self.normal_path, img_file))\n",
    "            resized = cv2.resize(img, (self.target_size[1], self.target_size[0]))\n",
    "            cv2.imwrite(os.path.join(normal_output, img_file), resized)\n",
    "\n",
    "    def enhance_violence_images(self):\n",
    "        \"\"\"Apply sharpening to violence images\"\"\"\n",
    "        print(\"\\nEnhancing violence images...\")\n",
    "        violence_dir = os.path.join(self.augmented_path, \"violence\")\n",
    "        \n",
    "        for img_file in tqdm(os.listdir(violence_dir), desc=\"Sharpening\"):\n",
    "            img_path = os.path.join(violence_dir, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            # Sharpening kernel\n",
    "            kernel = np.array([[0, -1, 0], \n",
    "                              [-1, 5, -1],\n",
    "                              [0, -1, 0]])\n",
    "            sharpened = cv2.filter2D(img, -1, kernel)\n",
    "            \n",
    "            cv2.imwrite(img_path, sharpened)\n",
    "\n",
    "    def augment_normal_images(self, n_to_augment=2000):\n",
    "        \"\"\"Add motion blur to subset of normal images\"\"\"\n",
    "        print(f\"\\nAugmenting {n_to_augment} normal images...\")\n",
    "        normal_dir = os.path.join(self.augmented_path, \"normal\")\n",
    "        normal_files = os.listdir(normal_dir)\n",
    "        \n",
    "        for img_file in tqdm(np.random.choice(normal_files, n_to_augment), desc=\"Motion blur\"):\n",
    "            img_path = os.path.join(normal_dir, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            # Motion blur kernel (horizontal)\n",
    "            kernel_size = 15\n",
    "            kernel = np.zeros((kernel_size, kernel_size))\n",
    "            kernel[int((kernel_size-1)/2), :] = np.ones(kernel_size)\n",
    "            kernel /= kernel_size\n",
    "            \n",
    "            blurred = cv2.filter2D(img, -1, kernel)\n",
    "            \n",
    "            # Save with new filename\n",
    "            base, ext = os.path.splitext(img_file)\n",
    "            cv2.imwrite(os.path.join(normal_dir, f\"{base}_blurred{ext}\"), blurred)\n",
    "\n",
    "    def create_annotations(self):\n",
    "        \"\"\"Create annotations for violence and normal images\"\"\"\n",
    "        print(\"\\nCreating annotations...\")\n",
    "        annotations = []\n",
    "\n",
    "        # Annotate violence images\n",
    "        violence_dir = os.path.join(self.augmented_path, \"violence\")\n",
    "        for img_file in tqdm(os.listdir(violence_dir), desc=\"Annotating violence images\"):\n",
    "            img_path = os.path.join(violence_dir, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            height, width = img.shape[:2]\n",
    "            \n",
    "            # Example bounding box (entire image for simplicity)\n",
    "            annotation = {\n",
    "                \"filename\": img_file,\n",
    "                \"class\": \"violence\",\n",
    "                \"bounding_boxes\": [{\"x_min\": 0, \"y_min\": 0, \"x_max\": width, \"y_max\": height}]\n",
    "            }\n",
    "            annotations.append(annotation)\n",
    "\n",
    "        # Annotate normal images\n",
    "        normal_dir = os.path.join(self.augmented_path, \"normal\")\n",
    "        for img_file in tqdm(os.listdir(normal_dir), desc=\"Annotating normal images\"):\n",
    "            img_path = os.path.join(normal_dir, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            height, width = img.shape[:2]\n",
    "            \n",
    "            # Example bounding box (entire image for simplicity)\n",
    "            annotation = {\n",
    "                \"filename\": img_file,\n",
    "                \"class\": \"normal\",\n",
    "                \"bounding_boxes\": [{\"x_min\": 0, \"y_min\": 0, \"x_max\": width, \"y_max\": height}]\n",
    "            }\n",
    "            annotations.append(annotation)\n",
    "\n",
    "        # Save annotations to JSON file\n",
    "        with open(os.path.join(self.annotations_path, \"annotations.json\"), \"w\") as f:\n",
    "            json.dump(annotations, f, indent=4)\n",
    "        print(f\"Annotations saved to {os.path.join(self.annotations_path, 'annotations.json')}\")\n",
    "\n",
    "    def verify_improvements(self):\n",
    "        \"\"\"Check final dataset quality\"\"\"\n",
    "        print(\"\\nVerifying optimized dataset...\")\n",
    "        violence_files = os.listdir(os.path.join(self.augmented_path, \"violence\"))\n",
    "        normal_files = os.listdir(os.path.join(self.augmented_path, \"normal\"))\n",
    "        \n",
    "        print(f\"\\nFinal Counts:\")\n",
    "        print(f\"Violence: {len(violence_files)} images\")\n",
    "        print(f\"Normal: {len(normal_files)} images\")\n",
    "        print(f\"Imbalance Ratio: {len(violence_files)/len(normal_files):.2f}:1\")\n",
    "        \n",
    "        # Show processed samples\n",
    "        self._display_samples(violence_files, normal_files)\n",
    "        \n",
    "        # Plot blur distribution comparison\n",
    "        self._plot_blur_comparison()\n",
    "\n",
    "    def _plot_blur_comparison(self):\n",
    "        \"\"\"Compare blur metrics before/after processing\"\"\"\n",
    "        print(\"\\nCalculating blur metrics...\")\n",
    "        \n",
    "        # Original violence blur\n",
    "        orig_violence_blur = []\n",
    "        for img_file in np.random.choice(os.listdir(self.violence_path), 500):\n",
    "            img = cv2.imread(os.path.join(self.violence_path, img_file))\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            orig_violence_blur.append(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
    "        \n",
    "        # Processed violence blur\n",
    "        proc_violence_blur = []\n",
    "        violence_dir = os.path.join(self.augmented_path, \"violence\")\n",
    "        for img_file in np.random.choice(os.listdir(violence_dir), 500):\n",
    "            img = cv2.imread(os.path.join(violence_dir, img_file))\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            proc_violence_blur.append(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
    "        \n",
    "        # Plot comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.kdeplot(orig_violence_blur, label='Original Violence', shade=True)\n",
    "        sns.kdeplot(proc_violence_blur, label='Enhanced Violence', shade=True)\n",
    "        plt.axvline(100, color='red', linestyle='--', label='Quality Threshold')\n",
    "        plt.title('Blur Metric Comparison: Before vs After Enhancement')\n",
    "        plt.xlabel('Laplacian Variance (Higher = Sharper)')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def prepare_training_data(self, test_size=0.2):\n",
    "        \"\"\"Create train/test splits with labels\"\"\"\n",
    "        print(\"\\nPreparing training data...\")\n",
    "        \n",
    "        # Load violence images\n",
    "        violence_dir = os.path.join(self.augmented_path, \"violence\")\n",
    "        violence_files = [os.path.join(violence_dir, f) for f in os.listdir(violence_dir)]\n",
    "        X_violence = [cv2.imread(f) for f in violence_files]\n",
    "        y_violence = np.ones(len(X_violence))\n",
    "        \n",
    "        # Load normal images\n",
    "        normal_dir = os.path.join(self.augmented_path, \"normal\")\n",
    "        normal_files = [os.path.join(normal_dir, f) for f in os.listdir(normal_dir)]\n",
    "        X_normal = [cv2.imread(f) for f in normal_files]\n",
    "        y_normal = np.zeros(len(X_normal))\n",
    "        \n",
    "        # Combine and split\n",
    "        X = np.array(X_violence + X_normal)\n",
    "        y = np.concatenate([y_violence, y_normal])\n",
    "        \n",
    "        return train_test_split(X, y, test_size=test_size, stratify=y)\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with your paths\n",
    "    optimizer = DatasetOptimizer(\n",
    "        violence_path=\"violence_dataset/violence\",\n",
    "        normal_path=\"violence_dataset/non_violence\"\n",
    "    )\n",
    "    \n",
    "    # Run optimization pipeline\n",
    "    optimizer.analyze_dataset()\n",
    "    optimizer.standardize_resolution()\n",
    "    optimizer.enhance_violence_images()\n",
    "    optimizer.augment_normal_images()\n",
    "    optimizer.create_annotations()  # Create annotations\n",
    "    optimizer.verify_improvements()\n",
    "    \n",
    "    # Get training data\n",
    "    X_train, X_test, y_train, y_test = optimizer.prepare_training_data()\n",
    "    \n",
    "    print(\"\\nDataset optimization complete!\")\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b426d-23a7-40e2-9421-93fa3a7fefac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a69a74-0869-4a97-b0a1-d46cccb7317b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
